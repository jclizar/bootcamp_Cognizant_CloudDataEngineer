{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Databricks Intro\r\n",
    "- criar conta https://community.cloud.databricks.com/\r\n",
    "- criar um cluster (manter a versao do runtime)\r\n",
    "- home>user>criar notebook = PysParkAula_pt1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PySpark\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "spark_session = SparkSession.builder.enableHiveSupport().getOrCreate()\r\n",
    "# Duas maneiras de acessar o contexto do spark a partir da sessão do spark\r\n",
    "spark_context = spark_session._sc\r\n",
    "spark_context = spark_session.sparkContext\r\n",
    "\r\n",
    "from pyspark.streaming import StreamingContext\r\n",
    "ssc = StreamingContext(sc, 1)\r\n",
    "lines = ssc.socketTextStream('localhost', 9999)\r\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\r\n",
    "counts.pprint()\r\n",
    "ssc.start()\r\n",
    "ssc.awaitTermination()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Leitura de Dataframe\r\n",
    "## Opção 1\r\n",
    "df1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(path_dataset1)\r\n",
    "## Opção 2\r\n",
    "df1 = spark.read.csv(path_dataset1)\r\n",
    "df1 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(path_dataset1)\r\n",
    "## Exibindo dataframe\r\n",
    "df1.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Outras formas de leitura de arquivos com PySpark\r\n",
    "path = \"/../../arquivoXPTO\"\r\n",
    "# Criando um dataframe a partir de um JSON\r\n",
    "dataframe = spark.read.json(path)\r\n",
    "# Criando um dataframe a partir de um ORC\r\n",
    "dataframe = spark.read.orc(path)\r\n",
    "# Criando um dataframe a partir de um PARQUET\r\n",
    "dataframe = spark.read.parquet(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Leitura de um RDD\r\n",
    "rdd = sc.textFile(path_rdd)\r\n",
    "#rdd.show() = Errado, não é possível exibir um SHOW() de um RDD, somente um Dataframe\r\n",
    "rdd.collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Criando uma tabela temporária\r\n",
    "nome_tabela_temporiaria = \"tempTableDataFrame1\"\r\n",
    "df1.createOrReplaceTempView(nome_tabela_temporiaria)\r\n",
    "\r\n",
    "nome_tabela_temporiaria = \"tempTableDataFrame1\"\r\n",
    "df1.createOrReplaceTempView(nome_tabela_temporiaria)\r\n",
    "# Lendo a tabela temporaria opcao 1\r\n",
    "spark.read.table(nome_tabela_temporiaria).show()\r\n",
    "# Lendo a tabela temporaria opcao 2\r\n",
    "spark.sql(\"SELECT * FROM tempTableDataFrame1\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualização do Databricks\r\n",
    "display(spark.sql(\"SELECT * FROM tempTableDataFrame1\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scala\r\n",
    "#import org.apache.spark.sql.functions._\r\n",
    "# Python\r\n",
    "from pyspark.sql.functions import col, column\r\n",
    "# Usando function col ou column\r\n",
    "df1.select(col(\"country\"), col(\"date\"), column(\"iso_code\")).show()\r\n",
    "# Usando selectExpr\r\n",
    "df1.selectExpr(\"country\", \"date\", \"iso_code\").show()\r\n",
    "\r\n",
    "# Scala import\r\n",
    "# org.apache.spark.sql.types._\r\n",
    "# Criando um Schema manualmente no PySpark\r\n",
    "from pyspark.sql.types import *\r\n",
    "dataframe_ficticio = StructType([\r\n",
    "StructField(\"col_String_1\", StringType()),\r\n",
    "StructField(\"col_Integer_2\", IntegerType()),\r\n",
    "StructField(\"col_Decimal_3\", DecimalType())\r\n",
    "])\r\n",
    "dataframe_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Função para gerar Schema (campos/colunas/nomes de colunas)\r\n",
    "'''\r\n",
    "# Scala\r\n",
    "org.apache.spark.sql.types._\r\n",
    "def getSchema(fields : Array[StructField]) : StructType = {\r\n",
    "new StructType(fields)\r\n",
    "}\r\n",
    "'''\r\n",
    "# PySpark\r\n",
    "def getSchema(fields):\r\n",
    "    return StructType(fields)\r\n",
    "schema = getSchema([StructField(\"coluna1\", StringType()), StructField(\"coluna2\", StringType()), StructField(\"coluna3\",\r\n",
    "StringType())])\r\n",
    "\r\n",
    "#Show\r\n",
    "df1.show(2)\r\n",
    "#Take\r\n",
    "df1.take(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gravando um novo CSV\r\n",
    "path_destino=\"/FileStore/tables/CSV/\"\r\n",
    "nome_arquivo=\"arquivo.csv\"\r\n",
    "path_geral= path_destino + nome_arquivo\r\n",
    "df1.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(path_geral)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gravando um novo JSON\r\n",
    "path_destino=\"/FileStore/tables/JSON/\"\r\n",
    "nome_arquivo=\"arquivo.json\"\r\n",
    "path_geral= path_destino + nome_arquivo\r\n",
    "df1.write.format(\"json\").mode(\"overwrite\").save(path_geral)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gravando um novo PARQUET\r\n",
    "path_destino=\"/FileStore/tables/PARQUET/\"\r\n",
    "nome_arquivo=\"arquivo.parquet\"\r\n",
    "path_geral= path_destino + nome_arquivo\r\n",
    "df1.write.format(\"parquet\").mode(\"overwrite\").save(path_geral)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Gravando um novo ORC\r\n",
    "path_destino=\"/FileStore/tables/ORC/\"\r\n",
    "nome_arquivo=\"arquivo.orc\"\r\n",
    "path_geral= path_destino + nome_arquivo\r\n",
    "df1.write.format(\"orc\").mode(\"overwrite\").save(path_geral)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Outros tipos de SELECT\r\n",
    "#Diferentes formas de selecionar uma coluna\r\n",
    "from pyspark.sql.functions import *\r\n",
    "df1.select(\"country\").show(5)\r\n",
    "df1.select('country').show(5)\r\n",
    "df1.select(col(\"country\")).show(5)\r\n",
    "df1.select(column(\"country\")).show(5)\r\n",
    "df1.select(expr(\"country\")).show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define uma nova coluna com um valor constante\r\n",
    "df2 = df1.withColumn(\"nova_coluna\", lit(1))\r\n",
    "# Adicionar coluna\r\n",
    "teste = expr(\"total_vaccinations < 40\")\r\n",
    "df1.select(\"country\", \"total_vaccinations\").withColumn(\"teste\", teste).show(5)\r\n",
    "# Renomear uma coluna\r\n",
    "df1.select(expr(\"total_vaccinations as total_de_vacinados\")).show(5)\r\n",
    "df1.select(col(\"country\").alias(\"pais\")).show(5)\r\n",
    "df1.select(\"country\").withColumnRenamed(\"country\", \"pais\").show(5)\r\n",
    "# Remover uma coluna\r\n",
    "df3 = df1.drop(\"country\")\r\n",
    "df3.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filtrando dados e ordenando\r\n",
    "# where() é um alias para filter().\r\n",
    "# Seleciona apenas os primeiros registros da coluna \"total_vaccinations\"\r\n",
    "df1.filter(df1.total_vaccinations > 55).orderBy(df1.total_vaccinations).show(2)\r\n",
    "# Filtra por país igual Argentina\r\n",
    "df1.select(df1.total_vaccinations, df1.country).filter(df1.country == \"Argentina\").show(5)\r\n",
    "# Filtra por país diferente Argentina\r\n",
    "df1.select(df1.total_vaccinations, df1.country).where(df1.country != \"Argentina\").show(5) # python type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filtrando dados e ordenando\r\n",
    "# Mostra valores únicos\r\n",
    "df1.select(\"country\").distinct().show()\r\n",
    "# Especificando vários filtros em comando separados\r\n",
    "filtro_vacinas = df1.total_vaccinations < 100\r\n",
    "filtro_pais = df1.country.contains(\"Argentina\")\r\n",
    "df1.select(df1.total_vaccinations, df1.country, df1.vaccines).where(df1.vaccines.isin(\"Sputnik V\", \"Sinovac\")).filter(filtro_vacinas).show(5)\r\n",
    "df1.select(df1.total_vaccinations, df1.country, df1.vaccines).where(df1.vaccines.isin(\"Sputnik V\",\r\n",
    "\"Sinovac\")).filter(filtro_vacinas).withColumn(\"filtro_pais\", filtro_pais).show(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"#######################################################################################################################\r\n",
    "Convertendo dados\r\n",
    "#######################################################################################################################\"\"\"\r\n",
    "df5 = df1.withColumn(\"PAIS\", col(\"country\").cast(\"string\").alias(\"PAIS\"))\r\n",
    "df5.select(df5.PAIS).show(2)\r\n",
    "\"\"\"#######################################################################################################################\r\n",
    "Trabalhando com funções\r\n",
    "#######################################################################################################################\"\"\"\r\n",
    "# Usando funções\r\n",
    "df1.select(upper(df1.country)).show(3)\r\n",
    "df1.select(lower(df1.country)).show(4"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}